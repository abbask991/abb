# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.
# removed invalid line
    https://colab.research.google.com/drive/1uXVdsWhJuWbtU3AyQ62yPQk6_vycHa4q
"""

import matplotlib.pyplot as plt
# removed invalid line
from datetime import datetime, timedelta
# removed invalid line
# Set timeframe
end_date = datetime.today()
start_date = end_date - timedelta(days=90)

tickers = {
    "Gold": "GC=F",
    "Oil": "CL=F",
    "EURUSD": "EURUSD=X",
    "GBPUSD": "GBPUSD=X"
}

# Asset tickers
tickers = {
    "Gold": "GC=F",
    "EUR/USD": "EURUSD=X",
    "Oil": "CL=F",
    "GBP/USD": "GBPUSD=X",
    "S&P 500": "^GSPC",
    "Dollar Index": "DX-Y.NYB"
}

# Download historical data
data = yf.download(list(tickers.values()), start=start_date, end=end_date)["Close"]
data.columns = tickers.keys()

# Print data and columns after download and renaming
print("Data after download and renaming:")
print(data.head())
print("Data columns after download and renaming:")
print(data.columns)

# Remove the 'Price' level from the MultiIndex
data.columns.name = None

# Print data and columns after removing MultiIndex level
print("Data after removing MultiIndex level:")
print(data.head())
print("Data columns after removing MultiIndex level:")
print(data.columns)


# Daily returns
returns = data.pct_change(fill_method=None).dropna()
print("Returns after pct_change and dropna:")
print(returns.head())


# Weekly returns
weekly_returns = data.resample("W").last().pct_change(fill_method=None).dropna()
print("Weekly returns after resample, pct_change and dropna:")
print(weekly_returns.head())


# Metrics table
metrics = pd.DataFrame({
    "Avg Daily Return": returns.mean(),
    "Weekly Return": weekly_returns.iloc[-1],
    "Volatility": returns.std(),
    "Sharpe Ratio": returns.mean() / returns.std()
})

print("ðŸ“‹ Market Metrics Summary")
print(metrics)

# --- Charting Section ---

# Price Chart
for asset in data.columns:
    plt.figure(figsize=(10, 4))
    plt.plot(data.index, data[asset])
    plt.title(f"{asset} - Price Chart")
    plt.xlabel("Date")
    plt.ylabel("Price")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Daily Returns
for asset in returns.columns:
    plt.figure(figsize=(10, 4))
    plt.plot(returns.index, returns[asset])
    plt.title(f"{asset} - Daily Returns")
    plt.xlabel("Date")
    plt.ylabel("Return")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Volatility (20-day rolling)
rolling_vol = returns.rolling(window=20).std()
plt.figure(figsize=(12, 5))
for asset in rolling_vol.columns:
    plt.plot(rolling_vol.index, rolling_vol[asset], label=asset)
plt.title("20-Day Rolling Volatility")
plt.xlabel("Date")
plt.ylabel("Volatility")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Cumulative Returns
cumulative_returns = (1 + returns).cumprod()
plt.figure(figsize=(12, 5))
for asset in cumulative_returns.columns:
    plt.plot(cumulative_returns.index, cumulative_returns[asset], label=asset)
plt.title("Cumulative Returns")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
# removed invalid line
from datetime import datetime, timedelta
# removed invalid line
# Define timeframe
end_date = datetime.today()
start_date = end_date - timedelta(days=180)

# Select symbol (Gold Futures)
symbol = "GC=F"
data = yf.download(symbol, start=start_date, end=end_date)

# Moving Averages
data["MA20"] = data["Close"].rolling(window=20).mean()
data["MA50"] = data["Close"].rolling(window=50).mean()

# Bollinger Bands
data["STD20"] = data["Close"].rolling(window=20).std()
data["UpperBB"] = data["MA20"] + 2 * data["STD20"]
data["LowerBB"] = data["MA20"] - 2 * data["STD20"]

# RSI Calculation
delta = data["Close"].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
rs = gain / loss
data["RSI"] = 100 - (100 / (1 + rs))

# MACD Calculation
ema12 = data["Close"].ewm(span=12, adjust=False).mean()
ema26 = data["Close"].ewm(span=26, adjust=False).mean()
data["MACD"] = ema12 - ema26
data["Signal"] = data["MACD"].ewm(span=9, adjust=False).mean()

# Print data before plotting
print("Data before plotting:")
print(data.tail())

# Plot: MA + Bollinger Bands
plt.figure(figsize=(12, 6))
plt.plot(data["Close"], label="Close", color="black")
plt.plot(data["MA20"], label="MA 20", linestyle='--')
plt.plot(data["MA50"], label="MA 50", linestyle='--')
plt.fill_between(data.index, data["UpperBB"], data["LowerBB"], color='gray', alpha=0.2, label="Bollinger Bands")
plt.title("Gold - Moving Averages & Bollinger Bands")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all') # Close the plot

# Plot: RSI
plt.figure(figsize=(12, 3))
plt.plot(data["RSI"], label="RSI", color="purple")
plt.axhline(70, color='red', linestyle='--', linewidth=1)
plt.axhline(30, color='green', linestyle='--', linewidth=1)
plt.title("Gold - Relative Strength Index (RSI)")
plt.xlabel("Date")
plt.ylabel("RSI")
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all') # Close the plot

# Plot: MACD
plt.figure(figsize=(12, 4))
plt.plot(data["MACD"], label="MACD", color="blue")
plt.plot(data["Signal"], label="Signal", color="orange")
plt.axhline(0, color='gray', linestyle='--')
plt.title("Gold - MACD")
plt.xlabel("Date")
plt.ylabel("MACD")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all') # Close the plot
# removed invalid line
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
# removed invalid line
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Fetch Gold price data
end_date = datetime.today()
start_date = end_date - timedelta(days=365)
symbol = "GC=F"
data = yf.download(symbol, start=start_date, end=end_date)

# Features
data["Return"] = data["Close"].pct_change()
data["MA10"] = data["Close"].rolling(window=10).mean()
data["MA50"] = data["Close"].rolling(window=50).mean()
data["Volatility"] = data["Return"].rolling(window=10).std()
data["RSI"] = 100 - (100 / (1 + data["Return"].rolling(14).apply(lambda x: (x[x > 0].mean() / abs(x[x < 0].mean())) if abs(x[x < 0].mean()) > 0 else 0)))

# Target: 1 = Buy, 0 = Hold, -1 = Sell
data["Signal"] = np.where(data["Return"].shift(-1) > 0.005, 1,
                   np.where(data["Return"].shift(-1) < -0.005, -1, 0))

# Prepare input/output
df = data[["Close", "MA10", "MA50", "Volatility", "RSI", "Signal"]].dropna()
X = df.drop(columns="Signal")
y = df["Signal"]

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Performance report
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

# Add predicted signals to DataFrame
df["Predicted Signal"] = model.predict(X)

# Plot: Buy/Sell Signal Chart
plt.figure(figsize=(12, 6))
plt.plot(df.index, df["Close"], label="Close Price", color="black")
plt.scatter(df[df["Predicted Signal"] == 1].index, df[df["Predicted Signal"] == 1]["Close"], label="Buy", marker="^", color="green")
plt.scatter(df[df["Predicted Signal"] == -1].index, df[df["Predicted Signal"] == -1]["Close"], label="Sell", marker="v", color="red")
plt.title("Gold - Model-Based Buy/Sell Signals")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
# removed invalid line
from datetime import datetime, timedelta
# removed invalid line
# Set the time range
end_date = datetime.today()
start_date = end_date - timedelta(days=365)

# Download historical asset prices
tickers = {
    "S&P 500": "^GSPC",
    "EUR/USD": "EURUSD=X",
    "Oil": "CL=F"
}
data = yf.download(list(tickers.values()), start=start_date, end=end_date)["Adj Close"]
data.columns = tickers.keys()

# Simulate macroeconomic data (replace with real sources like FRED for production use)
np.random.seed(42)
dates = pd.date_range(start=start_date, end=end_date, freq='M')
macro_data = pd.DataFrame({
    "Inflation Rate (%)": np.random.normal(loc=3.2, scale=0.3, size=len(dates)),
    "Interest Rate (%)": np.random.normal(loc=5.0, scale=0.2, size=len(dates)),
    "GDP Growth YoY (%)": np.random.normal(loc=2.1, scale=0.5, size=len(dates)),
    "Unemployment Rate (%)": np.random.normal(loc=3.8, scale=0.2, size=len(dates))
}, index=dates)

# Plot Macroeconomic Indicators
fig, axes = plt.subplots(2, 2, figsize=(14, 8))
macro_data["Inflation Rate (%)"].plot(ax=axes[0, 0], title="Inflation Rate (%)")
macro_data["Interest Rate (%)"].plot(ax=axes[0, 1], title="Interest Rate (%)", color="orange")
macro_data["GDP Growth YoY (%)"].plot(ax=axes[1, 0], title="GDP Growth YoY (%)", color="green")
macro_data["Unemployment Rate (%)"].plot(ax=axes[1, 1], title="Unemployment Rate (%)", color="red")

for ax in axes.flat:
    ax.set_xlabel("Date")
    ax.grid(True)

plt.tight_layout()
plt.show()

# Plot Asset Prices
plt.figure(figsize=(12, 5))
for asset in data.columns:
    plt.plot(data.index, data[asset], label=asset)
plt.title("Asset Prices: S&P 500, EUR/USD, Oil")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Show Macro Data
print("ðŸ“Š Macro Indicators:")
print(macro_data.tail())

"""# Task
Fix the errors in the provided Python code for a trading signal prediction model.

## Analyze model performance

### Subtask:
Examine the classification report and confusion matrix to understand the model's strengths and weaknesses for each class (-1, 0, 1).

**Reasoning**:
Analyze the classification report and confusion matrix to understand the model's performance for each class.
"""

from sklearn.metrics import confusion_matrix
# removed invalid line
import matplotlib.pyplot as plt

# Print the classification report again for easy reference
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

# Calculate and print the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n")
print(cm)

# Plot the confusion matrix for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Interpretation of the results
print("\nInterpretation:")
print("The classification report shows the precision, recall, and f1-score for each class:")
print("Class -1 (Sell): The model has low precision (0.18), meaning when it predicts a sell signal, it is correct only 18% of the time. The recall is also low (0.17), meaning it only identifies 17% of the actual sell opportunities. The f1-score (0.17) is low, indicating poor performance for this class.")
print("Class 0 (Hold): The model has slightly better precision (0.30) and recall (0.26) for hold signals, but the performance is still weak, as shown by the low f1-score (0.28).")
print("Class 1 (Buy): The model performs best on predicting buy signals with a precision of 0.40 and a recall of 0.46. The f1-score (0.43) is the highest among the three classes, indicating relatively better performance, though still not very strong.")
print("\nThe confusion matrix provides a detailed breakdown of predictions:")
print(f"True Negatives (Correctly predicted -1): {cm[0, 0]}")
print(f"False Positives (Predicted 0 when actual is -1): {cm[0, 1]}")
print(f"False Positives (Predicted 1 when actual is -1): {cm[0, 2]}")
print(f"False Negatives (Predicted -1 when actual is 0): {cm[1, 0]}")
print(f"True Negatives (Correctly predicted 0): {cm[1, 1]}")
print(f"False Positives (Predicted 1 when actual is 0): {cm[1, 2]}")
print(f"False Negatives (Predicted -1 when actual is 1): {cm[2, 0]}")
print(f"False Negatives (Predicted 0 when actual is 1): {cm[2, 1]}")
print(f"True Positives (Correctly predicted 1): {cm[2, 2]}")
print("\nSummary of Performance:")
print("The model is struggling significantly with predicting 'Sell' (-1) and 'Hold' (0) signals, as indicated by the low precision, recall, and f1-scores, and the high number of misclassifications in the confusion matrix. It performs relatively better, but still weakly, on predicting 'Buy' (1) signals. Overall, the model's accuracy is low (0.33), suggesting it's not reliably predicting trading signals based on the current features and model.")

"""## Feature engineering

### Subtask:
Explore creating new features from the existing data that might provide more predictive power for the trading signals.

**Reasoning**:
The previous cell failed because the yfinance download did not return an 'Adj Close' column. To fix this, I will remove the slicing by 'Adj Close'. Additionally, I will implement feature engineering steps outlined in the subtask, including calculating additional technical indicators, creating lag features, adding time-based features, and merging with macroeconomic data.
"""

from sklearn.preprocessing import StandardScaler

# Set the time range
end_date = datetime.today()
start_date = end_date - timedelta(days=365)

# Download historical asset prices
tickers = {
    "S&P 500": "^GSPC",
    "EUR/USD": "EURUSD=X",
    "Oil": "CL=F",
    "Gold": "GC=F" # Include Gold for merging with macro data later
}
data = yf.download(list(tickers.values()), start=start_date, end=end_date)["Close"] # Removed ["Adj Close"]
data.columns = tickers.keys()

# Simulate macroeconomic data (replace with real sources like FRED for production use)
np.random.seed(42)
macro_dates = pd.date_range(start=start_date, end=end_date, freq='M')
macro_data = pd.DataFrame({
    "Inflation Rate (%)": np.random.normal(loc=3.2, scale=0.3, size=len(macro_dates)),
    "Interest Rate (%)": np.random.normal(loc=5.0, scale=0.2, size=len(macro_dates)),
    "GDP Growth YoY (%)": np.random.normal(loc=2.1, scale=0.5, size=len(macro_dates)),
    "Unemployment Rate (%)": np.random.normal(loc=3.8, scale=0.2, size=len(macro_dates))
}, index=macro_dates)

# Select Gold data for feature engineering
gold_data = data["Gold"].copy().to_frame()

# Original Features (from the previous cell)
gold_data["Return"] = gold_data["Gold"].pct_change()
gold_data["MA10"] = gold_data["Gold"].rolling(window=10).mean()
gold_data["MA50"] = gold_data["Gold"].rolling(window=50).mean()
gold_data["Volatility"] = gold_data["Return"].rolling(window=10).std()
gold_data["RSI"] = 100 - (100 / (1 + gold_data["Return"].rolling(14).apply(lambda x: (x[x > 0].mean() / abs(x[x < 0].mean())) if abs(x[x < 0].mean()) > 0 else 0)))

# Target: 1 = Buy, 0 = Hold, -1 = Sell (using Gold price)
gold_data["Signal"] = np.where(gold_data["Return"].shift(-1) > 0.005, 1,
                   np.where(gold_data["Return"].shift(-1) < -0.005, -1, 0))

# Additional Technical Indicators (Step 1)
# Bollinger Bands
gold_data["MA20"] = gold_data["Gold"].rolling(window=20).mean()
gold_data["STD20"] = gold_data["Gold"].rolling(window=20).std()
gold_data["Bollinger_Band_Width"] = (gold_data["MA20"] + 2 * gold_data["STD20"]) - (gold_data["MA20"] - 2 * gold_data["STD20"])

# MACD Histogram
ema12 = gold_data["Gold"].ewm(span=12, adjust=False).mean()
ema26 = gold_data["Gold"].ewm(span=26, adjust=False).mean()
macd = ema12 - ema26
signal_line = macd.ewm(span=9, adjust=False).mean()
gold_data["MACD_Histogram"] = macd - signal_line

# Stochastic Oscillator
low_14 = gold_data["Gold"].rolling(window=14).min()
high_14 = gold_data["Gold"].rolling(window=14).max()
gold_data["%K"] = 100 * ((gold_data["Gold"] - low_14) / (high_14 - low_14))
gold_data["%D"] = gold_data["%K"].rolling(window=3).mean()

# Lag Features (Step 2)
gold_data["Return_Lag1"] = gold_data["Return"].shift(1)
gold_data["MA10_Lag1"] = gold_data["MA10"].shift(1)

# Interaction Terms (Step 3 - Example: Return * Volatility)
gold_data["Return_Volatility_Interaction"] = gold_data["Return"] * gold_data["Volatility"]

# Time-based Features (Step 4)
gold_data["Day_of_Week"] = gold_data.index.dayofweek
gold_data["Month"] = gold_data.index.month

# Include macroeconomic indicators (Step 5)
# Resample macro data to daily and forward fill missing values
macro_daily = macro_data.resample('D').ffill()

# Merge macro data with gold data
df_features = gold_data.merge(macro_daily, left_index=True, right_index=True, how='left')

# Drop rows with missing values resulting from feature creation or merging (Step 6)
df_features.dropna(inplace=True)

# Display the first few rows of the new DataFrame with features
print("DataFrame with engineered features:")
display(df_features.head())

# Display the columns to confirm new features are present
print("\nColumns of the DataFrame with engineered features:")
print(df_features.columns)

"""## Model selection and tuning

### Subtask:
Experiment with different classification models or tune the hyperparameters of the current RandomForestClassifier to potentially improve results.

**Reasoning**:
Define features and target, split the data, train a RandomForestClassifier with initial hyperparameters, predict, and evaluate using classification report and confusion matrix. Then, tune the hyperparameters using GridSearchCV and evaluate the tuned model.
"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
# removed invalid line
import matplotlib.pyplot as plt

# 1. Define features (X) and target (y)
# Exclude the original 'Gold' price column as it's not a feature for predicting the signal
# Also exclude the original 'Signal' column from features
X = df_features.drop(columns=["Gold", "Signal"])
y = df_features["Signal"]

# 2. Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Instantiate and 4. Train initial RandomForestClassifier
print("Training initial RandomForestClassifier...")
initial_model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=3, random_state=42)
initial_model.fit(X_train, y_train)

# 5. Predict and 6. Evaluate initial model
y_pred_initial = initial_model.predict(X_test)
print("\nInitial RandomForestClassifier Performance Report:")
print(classification_report(y_test, y_pred_initial))
cm_initial = confusion_matrix(y_test, y_pred_initial)
print("\nInitial Confusion Matrix:\n")
print(cm_initial)

# Plot the initial confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_initial, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Initial RandomForestClassifier Confusion Matrix")
plt.show()
plt.close('all') # Close the plot

# 7. Experiment with hyperparameter tuning using GridSearchCV
print("\nTuning RandomForestClassifier with GridSearchCV...")
# Define the parameter grid to search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 3, 5]
}

# Instantiate GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best parameters and best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

print("\nBest Hyperparameters found by GridSearchCV:")
print(best_params)

# 8. Re-evaluate the tuned model
print("\nTuned RandomForestClassifier Performance Report:")
y_pred_tuned = best_model.predict(X_test)
print(classification_report(y_test, y_pred_tuned))
cm_tuned = confusion_matrix(y_test, y_pred_tuned)
print("\nTuned Confusion Matrix:\n")
print(cm_tuned)

# Plot the tuned confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_tuned, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Tuned RandomForestClassifier Confusion Matrix")
plt.show()
plt.close('all') # Close the plot

# 9. Compare performance metrics (done by visual inspection of reports)
# 10. Select the best model (the tuned model based on f1_weighted score)
print("\nComparison and Selection:")
print("Comparing the classification reports and confusion matrices of the initial and tuned models.")
print(f"Initial Model Weighted F1-score: {classification_report(y_test, y_pred_initial, output_dict=True)['weighted avg']['f1-score']:.4f}")
print(f"Tuned Model Weighted F1-score: {classification_report(y_test, y_pred_tuned, output_dict=True)['weighted avg']['f1-score']:.4f}")
print("\nThe tuned model with the best hyperparameters found by GridSearchCV is selected based on the weighted average F1-score.")

"""**Reasoning**:
Experiment with a different classification model, specifically a GradientBoostingClassifier, to see if it can improve performance compared to the RandomForestClassifier.


"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
# removed invalid line
import matplotlib.pyplot as plt

# Instantiate and train GradientBoostingClassifier
print("\nTraining GradientBoostingClassifier...")
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)

# Predict and evaluate GradientBoostingClassifier
y_pred_gb = gb_model.predict(X_test)
print("\nGradientBoostingClassifier Performance Report:")
print(classification_report(y_test, y_pred_gb))
cm_gb = confusion_matrix(y_test, y_pred_gb)
print("\nGradientBoostingClassifier Confusion Matrix:\n")
print(cm_gb)

# Plot the GradientBoostingClassifier confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_gb, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("GradientBoostingClassifier Confusion Matrix")
plt.show()
plt.close('all') # Close the plot

# Compare performance (done by visual inspection of reports)
# We will compare this to the tuned RandomForestClassifier performance.
print("\nComparison with Tuned RandomForestClassifier:")
# The metrics for the tuned RandomForestClassifier are available in the previous output.
# Visually compare the classification reports and confusion matrices.

"""## Cross-validation

### Subtask:
Implement cross-validation to get a more robust estimate of the model's performance and avoid overfitting.

**Reasoning**:
Implement 5-fold cross-validation using GradientBoostingClassifier and print the results.
"""

from sklearn.model_selection import cross_val_score

# Assuming 'gb_model' is the best performing model from the previous step
# Use cross_val_score with cv=5 and scoring='f1_weighted'
cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1_weighted')

# Print the cross-validation scores
print("Cross-validation F1-weighted scores:", cv_scores)

# Calculate and print the mean and standard deviation of the cross-validation scores
print("Mean F1-weighted score:", cv_scores.mean())
print("Standard deviation of F1-weighted scores:", cv_scores.std())

"""## Backtesting (optional but recommended)

### Subtask:
If possible with the available data and libraries, perform a basic backtest to simulate the trading strategy based on the model's signals and evaluate its performance on historical data.

**Reasoning**:
Perform a basic backtest simulation and evaluate the strategy's performance against a buy-and-hold benchmark.
"""

# Create a new DataFrame for backtesting
df_backtest = df_features.copy()

# Calculate strategy returns
df_backtest['Strategy Returns'] = df_backtest['Return'] * df_backtest['Signal']

# Calculate cumulative returns for the strategy
df_backtest['Cumulative Strategy Returns'] = (1 + df_backtest['Strategy Returns']).cumprod()

# Calculate cumulative returns for the buy-and-hold benchmark (using Gold's return)
df_backtest['Cumulative Buy and Hold Returns'] = (1 + df_backtest['Return']).cumprod()

# Plot cumulative returns
plt.figure(figsize=(12, 6))
plt.plot(df_backtest.index, df_backtest['Cumulative Strategy Returns'], label='Strategy Returns', color='blue')
plt.plot(df_backtest.index, df_backtest['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy vs. Buy and Hold Benchmark')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Print key performance metrics
strategy_total_return = df_backtest['Cumulative Strategy Returns'].iloc[-1] - 1
benchmark_total_return = df_backtest['Cumulative Buy and Hold Returns'].iloc[-1] - 1

print("\nðŸ“ˆ Backtest Performance Summary:")
print(f"Total Cumulative Strategy Return: {strategy_total_return:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""## Refine and iterate

### Subtask:
Refine the model and features based on the analysis and results from the previous steps and repeat the process as needed.

**Reasoning**:
Address class imbalance and retrain the Gradient Boosting Classifier.
"""

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
# removed invalid line
import matplotlib.pyplot as plt

# Separate features and target again after feature engineering and cleaning
X = df_features.drop(columns="Signal")
y = df_features["Signal"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE for oversampling
smote = SMOTE(random_state=42, k_neighbors=1) # Reduced k_neighbors
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Re-train the Gradient Boosting Classifier model using the balanced dataset
print("Training GradientBoostingClassifier with SMOTE...")
gb_model_resampled = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model_resampled.fit(X_train_resampled, y_train_resampled)

# Evaluate the performance of the refined model
y_pred_resampled = gb_model_resampled.predict(X_test)
print("\nGradientBoostingClassifier Performance Report (with SMOTE):")
print(classification_report(y_test, y_pred_resampled))

cm_resampled = confusion_matrix(y_test, y_pred_resampled)
print("\nGradientBoostingClassifier Confusion Matrix (with SMOTE):\n")
print(cm_resampled)

# Plot the confusion matrix for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm_resampled, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("GradientBoostingClassifier Confusion Matrix (with SMOTE)")
plt.show()

"""**Reasoning**:
The previous code failed because SMOTE requires the number of neighbors to be less than or equal to the number of samples in the minority class. Given the small dataset size, reducing the number of neighbors for SMOTE should resolve this issue.


"""

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
# removed invalid line
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Separate features and target again after feature engineering and cleaning
X = df_features.drop(columns="Signal")
y = df_features["Signal"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE for oversampling
# Reduce k_neighbors to be less than the smallest class size in the training data
smote = SMOTE(random_state=42, k_neighbors=1) # Reduced k_neighbors
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Re-train the Gradient Boosting Classifier model using the balanced dataset
print("Training GradientBoostingClassifier with SMOTE...")
gb_model_resampled = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model_resampled.fit(X_train_resampled, y_train_resampled)

# Evaluate the performance of the refined model
y_pred_resampled = gb_model_resampled.predict(X_test)
print("\nGradientBoostingClassifier Performance Report (with SMOTE):")
print(classification_report(y_test, y_pred_resampled))

cm_resampled = confusion_matrix(y_test, y_pred_resampled)
print("\nGradientBoostingClassifier Confusion Matrix (with SMOTE):\n")
print(cm_resampled)

# Plot the confusion matrix for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm_resampled, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("GradientBoostingClassifier Confusion Matrix (with SMOTE)")
plt.show()

"""**Reasoning**:
Evaluate the refined model using cross-validation and backtesting to get a more robust estimate of its performance.


"""

from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
# removed invalid line
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split


# Separate features and target again after new feature engineering and cleaning
X = df_features.drop(columns="Signal")
y = df_features["Signal"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE for oversampling on the new training data
# Reduce k_neighbors if necessary based on the smallest class size
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Re-train the Gradient Boosting Classifier model using the balanced dataset and new features
print("Training GradientBoostingClassifier with SMOTE and advanced features...")
gb_model_resampled = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model_resampled.fit(X_train_resampled, y_train_resampled)


# Use cross_val_score with cv=5 and scoring='f1_weighted' on the resampled model
cv_scores_resampled = cross_val_score(gb_model_resampled, X, y, cv=5, scoring='f1_weighted')

# Print the cross-validation scores
print("\nCross-validation F1-weighted scores (with SMOTE):", cv_scores_resampled)

# Calculate and print the mean and standard deviation of the cross-validation scores
print("Mean F1-weighted score (with SMOTE):", cv_scores_resampled.mean())
print("Standard deviation of F1-weighted scores (with SMOTE):", cv_scores_resampled.std())

# Backtesting with the refined model
# Create a new DataFrame for backtesting
df_backtest_resampled = df_features.copy()

# Predict signals using the resampled model on the full dataset
df_backtest_resampled['Predicted Signal'] = gb_model_resampled.predict(X)

# Calculate strategy returns based on the refined model's signals
df_backtest_resampled['Strategy Returns (Resampled)'] = df_backtest_resampled['Return'] * df_backtest_resampled['Predicted Signal']

# Calculate cumulative returns for the strategy
df_backtest_resampled['Cumulative Strategy Returns (Resampled)'] = (1 + df_backtest_resampled['Strategy Returns (Resampled)']).cumprod()

# Calculate cumulative returns for the buy-and-hold benchmark (using Gold's return)
df_backtest_resampled['Cumulative Buy and Hold Returns'] = (1 + df_backtest_resampled['Return']).cumprod()


# Plot cumulative returns
plt.figure(figsize=(12, 6))
plt.plot(df_backtest_resampled.index, df_backtest_resampled['Cumulative Strategy Returns (Resampled)'], label='Strategy Returns (with SMOTE)', color='blue')
plt.plot(df_backtest_resampled.index, df_backtest_resampled['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy (with SMOTE) vs. Buy and Hold Benchmark')
plt.xlabel("Date")
plt.ylabel("Cumulative Returns")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Print key performance metrics
strategy_total_return_resampled = df_backtest_resampled['Cumulative Strategy Returns (Resampled)'].iloc[-1] - 1
benchmark_total_return = df_backtest_resampled['Cumulative Buy and Hold Returns'].iloc[-1] - 1


print("\nðŸ“ˆ Backtest Performance Summary (with SMOTE):")
print(f"Total Cumulative Strategy Return (with SMOTE): {strategy_total_return_resampled:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""**Reasoning**:
The previous code failed because the 'Cumulative Buy and Hold Returns' column was not present in the `df_backtest_resampled` DataFrame. I need to recreate this column and the 'Buy and Hold Returns' plot.


"""

from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
# removed invalid line
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split


# Use cross_val_score with cv=5 and scoring='f1_weighted' on the resampled model
cv_scores_resampled = cross_val_score(gb_model_resampled, X, y, cv=5, scoring='f1_weighted')

# Print the cross-validation scores
print("\nCross-validation F1-weighted scores (with SMOTE):", cv_scores_resampled)

# Calculate and print the mean and standard deviation of the cross-validation scores
print("Mean F1-weighted score (with SMOTE):", cv_scores_resampled.mean())
print("Standard deviation of F1-weighted scores (with SMOTE):", cv_scores_resampled.std())

# Backtesting with the refined model
# Create a new DataFrame for backtesting
df_backtest_resampled = df_features.copy()

# Predict signals using the resampled model on the full dataset
df_backtest_resampled['Predicted Signal'] = gb_model_resampled.predict(X)

# Calculate strategy returns based on the refined model's signals
df_backtest_resampled['Strategy Returns (Resampled)'] = df_backtest_resampled['Return'] * df_backtest_resampled['Predicted Signal']

# Calculate cumulative returns for the strategy
df_backtest_resampled['Cumulative Strategy Returns (Resampled)'] = (1 + df_backtest_resampled['Strategy Returns (Resampled)']).cumprod()

# Calculate cumulative returns for the buy-and-hold benchmark (using Gold's return)
df_backtest_resampled['Cumulative Buy and Hold Returns'] = (1 + df_backtest_resampled['Return']).cumprod()


# Plot cumulative returns
plt.figure(figsize=(12, 6))
plt.plot(df_backtest_resampled.index, df_backtest_resampled['Cumulative Strategy Returns (Resampled)'], label='Strategy Returns (with SMOTE)', color='blue')
plt.plot(df_backtest_resampled.index, df_backtest_resampled['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy (with SMOTE) vs. Buy and Hold Benchmark')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Print key performance metrics
strategy_total_return_resampled = df_backtest_resampled['Cumulative Strategy Returns (Resampled)'].iloc[-1] - 1
benchmark_total_return = df_backtest_resampled['Cumulative Buy and Hold Returns'].iloc[-1] - 1


print("\nðŸ“ˆ Backtest Performance Summary (with SMOTE):")
print(f"Total Cumulative Strategy Return (with SMOTE): {strategy_total_return_resampled:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""## Refine and iterate

### Subtask:
Refine the model and features based on the analysis and results from the previous steps and repeat the process as needed.

**Reasoning**:
Analyze the classification report, confusion matrix, cross-validation, and backtesting results to identify potential areas for further refinement and plan the next steps.
"""

# Analyze the classification report and confusion matrix
print("Analysis of Classification Report (with SMOTE):")
print("Precision, Recall, and F1-score for each class:")
print("- Class -1 (Sell): Precision, Recall, and F1-score are still low, indicating poor performance in identifying sell signals.")
print("- Class 0 (Hold): Performance is slightly better than for sell signals but still weak.")
print("- Class 1 (Buy): Shows the highest precision and recall among the three classes, but overall performance is still not strong.")

print("\nAnalysis of Confusion Matrix (with SMOTE):")
print("The confusion matrix shows that the model still misclassifies a significant number of instances across all classes.")
print("There's an improvement in predicting some instances of minority classes (-1 and 1) compared to the model without SMOTE, but the number of false positives and false negatives is still high.")


# Evaluate cross-validation results
print("\nAnalysis of Cross-validation Results (with SMOTE):")
print(f"Mean F1-weighted score: {cv_scores_resampled.mean():.4f}")
print(f"Standard deviation of F1-weighted scores: {cv_scores_resampled.std():.4f}")
print("The mean F1-weighted score is low, and the standard deviation is relatively high, indicating inconsistent and generally poor performance across different data subsets.")
print("Compared to the cross-validation results without SMOTE, there's a slight improvement in the mean F1-weighted score, but the performance is still far from satisfactory.")

# Analyze backtesting results
print("\nAnalysis of Backtest Performance Summary (with SMOTE):")
print(f"Total Cumulative Strategy Return (with SMOTE): {strategy_total_return_resampled:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")
print("The strategy with SMOTE slightly underperformed the simple buy-and-hold benchmark during the backtest period.")
print("The cumulative returns plot visually confirms this underperformance and the lack of consistent upward trend in the strategy's returns.")

# Identify potential areas for further refinement and plan next steps
print("\nPotential Areas for Further Refinement:")
print("1. Feature Engineering: The current features might not be sufficiently predictive of the trading signals. Explore creating more advanced features, such as: ")
print("   - Incorporating macroeconomic indicators more effectively (e.g., using lagged macro data, or combining macro indicators with price data).")
print("   - Creating interaction terms between technical indicators and macro indicators.")
print("   - Exploring alternative technical indicators or optimizing the parameters of existing ones (e.g., lookback periods for MAs, RSI, etc.).")
print("   - Considering sentiment analysis or news-based features if relevant data is available.")
print("2. Model Selection and Tuning: The Gradient Boosting Classifier might not be the most suitable model, or its hyperparameters need further tuning.")
print("   - Experiment with other classification algorithms suitable for time series data (e.g., LSTM, ARIMA-based models, other tree-based models like XGBoost or LightGBM).")
print("   - Perform more extensive hyperparameter tuning for the Gradient Boosting Classifier using techniques like RandomizedSearchCV or Bayesian Optimization.")
print("3. Handling Class Imbalance: While SMOTE was used, the results are still not great. Explore alternative or complementary techniques for handling class imbalance:")
print("   - Experiment with different sampling strategies (e.g., undersampling the majority class, or a combination of oversampling and undersampling).")
print("   - Consider using evaluation metrics that are less sensitive to class imbalance (e.g., balanced accuracy).")
print("   - Explore cost-sensitive learning, where misclassifications of minority classes are penalized more heavily.")
print("4. Target Definition: Revisit the definition of the trading signal target variable. The current thresholds (0.005 and -0.005) might be too strict or not optimal.")
print("   - Experiment with different return thresholds for defining buy/sell signals.")
print("   - Consider alternative target variables (e.g., predicting the direction of the next day's price movement instead of discrete buy/hold/sell signals).")
print("5. Time Series Considerations: The current approach treats the data as independent samples. Explore methods that explicitly account for the temporal nature of time series data:")
print("   - Use time series cross-validation to ensure that the model is evaluated on future data.")
print("   - Incorporate time series features (e.g., autocorrelation, partial autocorrelation).")

print("\nNext Steps:")
print("Based on the analysis, the next step will be to perform more advanced feature engineering, focusing on incorporating macroeconomic data and creating interaction terms. Following that, we will retrain the model with the new features, re-evaluate using cross-validation and backtesting, and continue iterating on model tuning or exploring alternative algorithms as needed.")

"""**Reasoning**:
Based on the analysis, the next step is to perform more advanced feature engineering by incorporating lagged macroeconomic data and interaction terms between technical and macroeconomic indicators.


"""

# Incorporate lagged macroeconomic data
# Create lagged features for macroeconomic indicators
macro_cols = ["Inflation Rate (%)", "Interest Rate (%)", "GDP Growth YoY (%)", "Unemployment Rate (%)"]
for col in macro_cols:
    df_features[f"{col}_Lag1"] = df_features[col].shift(1)
    df_features[f"{col}_Lag2"] = df_features[col].shift(2)
    df_features[f"{col}_Lag3"] = df_features[col].shift(3) # Example of adding more lags

# Create interaction terms between technical and macroeconomic indicators
# Example interactions (can be expanded)
df_features["MA10_Inflation_Interaction"] = df_features["MA10"] * df_features["Inflation Rate (%)"]
df_features["Volatility_Interest_Interaction"] = df_features["Volatility"] * df_features["Interest Rate (%)"]
df_features["RSI_Unemployment_Interaction"] = df_features["RSI"] * df_features["Unemployment Rate (%)"]

# Drop rows with new missing values resulting from lagged features
df_features.dropna(inplace=True)

# Display the first few rows of the DataFrame with new engineered features
print("DataFrame with advanced engineered features:")
display(df_features.head())

# Display the columns to confirm new features are present
print("\nColumns of the DataFrame with advanced engineered features:")
print(df_features.columns)

"""**Reasoning**:
Retrain the Gradient Boosting Classifier model with the new set of engineered features and re-evaluate its performance using the classification report, confusion matrix, cross-validation, and backtesting.


"""

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, make_scorer
# removed invalid line
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score

# Separate features and target again after new feature engineering and cleaning
X = df_features.drop(columns="Signal")
y = df_features["Signal"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE for oversampling on the new training data
# Reduce k_neighbors if necessary based on the smallest class size
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Re-train the Gradient Boosting Classifier model using the balanced dataset and new features
print("Training GradientBoostingClassifier with SMOTE and advanced features...")
gb_model_refined = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model_refined.fit(X_train_resampled, y_train_resampled)

# Evaluate the performance of the refined model on the test set
y_pred_refined = gb_model_refined.predict(X_test)
print("\nGradientBoostingClassifier Performance Report (Refined Model):")
print(classification_report(y_test, y_pred_refined))

cm_refined = confusion_matrix(y_test, y_pred_refined)
print("\nGradientBoostingClassifier Confusion Matrix (Refined Model):\n")
print(cm_refined)

# Plot the confusion matrix for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm_refined, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("GradientBoostingClassifier Confusion Matrix (Refined Model)")
plt.show()
plt.close('all') # Close the plot

# Cross-validation with the refined model
# Use make_scorer to specify the f1_weighted scoring
weighted_f1_scorer = make_scorer(f1_score, average='weighted')
cv_scores_refined = cross_val_score(gb_model_refined, X, y, cv=5, scoring=weighted_f1_scorer)


print("\nCross-validation F1-weighted scores (Refined Model):", cv_scores_refined)
print("Mean F1-weighted score (Refined Model):", cv_scores_refined.mean())
print("Standard deviation of F1-weighted scores (Refined Model):", cv_scores_refined.std())

# Backtesting with the refined model
df_backtest_refined = df_features.copy()
df_backtest_refined['Predicted Signal'] = gb_model_refined.predict(X)
df_backtest_refined['Strategy Returns (Refined)'] = df_backtest_refined['Return'] * df_backtest_refined['Predicted Signal']
df_backtest_refined['Cumulative Strategy Returns (Refined)'] = (1 + df_backtest_refined['Strategy Returns (Refined)']).cumprod()
df_backtest_refined['Cumulative Buy and Hold Returns'] = (1 + df_backtest_refined['Return']).cumprod()

plt.figure(figsize=(12, 6))
plt.plot(df_backtest_refined.index, df_backtest_refined['Cumulative Strategy Returns (Refined)'], label='Strategy Returns (Refined)', color='blue')
plt.plot(df_backtest_refined.index, df_backtest_refined['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy (Refined Model) vs. Buy and Hold Benchmark')
plt.xlabel("Date")
plt.ylabel("Cumulative Returns")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all') # Close the plot

strategy_total_return_refined = df_backtest_refined['Cumulative Strategy Returns (Refined)'].iloc[-1] - 1
benchmark_total_return = df_backtest_refined['Cumulative Buy and Hold Returns'].iloc[-1] - 1

print("\nðŸ“ˆ Backtest Performance Summary (Refined Model):")
print(f"Total Cumulative Strategy Return (Refined Model): {strategy_total_return_refined:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""**Reasoning**:
The previous code block failed due to an ImportError. The `f1_weighted` metric is not directly importable from `sklearn.metrics`. It is a scoring option used in functions like `cross_val_score`. The import statement needs to be corrected.


"""

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, make_scorer
# removed invalid line
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score

# Separate features and target again after new feature engineering and cleaning
X = df_features.drop(columns="Signal")
y = df_features["Signal"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE for oversampling on the new training data
# Reduce k_neighbors if necessary based on the smallest class size
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Re-train the Gradient Boosting Classifier model using the balanced dataset and new features
print("Training GradientBoostingClassifier with SMOTE and advanced features...")
gb_model_refined = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model_refined.fit(X_train_resampled, y_train_resampled)

# Evaluate the performance of the refined model on the test set
y_pred_refined = gb_model_refined.predict(X_test)
print("\nGradientBoostingClassifier Performance Report (Refined Model):")
print(classification_report(y_test, y_pred_refined))

cm_refined = confusion_matrix(y_test, y_pred_refined)
print("\nGradientBoostingClassifier Confusion Matrix (Refined Model):\n")
print(cm_refined)

# Plot the confusion matrix for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm_refined, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("GradientBoostingClassifier Confusion Matrix (Refined Model)")
plt.show()
plt.close('all') # Close the plot

# Cross-validation with the refined model
# Use make_scorer to specify the f1_weighted scoring
weighted_f1_scorer = make_scorer(f1_score, average='weighted')
cv_scores_refined = cross_val_score(gb_model_refined, X, y, cv=5, scoring=weighted_f1_scorer)


print("\nCross-validation F1-weighted scores (Refined Model):", cv_scores_refined)
print("Mean F1-weighted score (Refined Model):", cv_scores_refined.mean())
print("Standard deviation of F1-weighted scores (Refined Model):", cv_scores_refined.std())

# Backtesting with the refined model
df_backtest_refined = df_features.copy()
df_backtest_refined['Predicted Signal'] = gb_model_refined.predict(X)
df_backtest_refined['Strategy Returns (Refined)'] = df_backtest_refined['Return'] * df_backtest_refined['Predicted Signal']
df_backtest_refined['Cumulative Strategy Returns (Refined)'] = (1 + df_backtest_refined['Strategy Returns (Refined)']).cumprod()
df_backtest_refined['Cumulative Buy and Hold Returns'] = (1 + df_backtest_refined['Return']).cumprod()

plt.figure(figsize=(12, 6))
plt.plot(df_backtest_refined.index, df_backtest_refined['Cumulative Strategy Returns (Refined)'], label='Strategy Returns (Refined)', color='blue')
plt.plot(df_backtest_refined.index, df_backtest_refined['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy (Refined Model) vs. Buy and Hold Benchmark')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all') # Close the plot

strategy_total_return_refined = df_backtest_refined['Cumulative Strategy Returns (Refined)'].iloc[-1] - 1
benchmark_total_return = df_backtest_refined['Cumulative Buy and Hold Returns'].iloc[-1] - 1

print("\nðŸ“ˆ Backtest Performance Summary (Refined Model):")
print(f"Total Cumulative Strategy Return (Refined Model): {strategy_total_return_refined:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""**Reasoning**:
Analyze the results from the refined model with advanced features and SMOTE, compare them to the previous iterations, and determine the next steps for further improvement based on the identified areas from the prior analysis.


"""

# Analyze the classification report and confusion matrix of the refined model
print("Analysis of Classification Report (Refined Model with Advanced Features and SMOTE):")
print("Precision, Recall, and F1-score for each class on the test set:")
print("- Class -1 (Sell): Precision, Recall, and F1-score are still very low (0.00), indicating that the model is not effectively predicting sell signals.")
print("- Class 0 (Hold): Performance remains weak, with low precision and recall.")
print("- Class 1 (Buy): Performance is similar to the previous iteration, still not strong.")

print("\nAnalysis of Confusion Matrix (Refined Model with Advanced Features and SMOTE):")
print("The confusion matrix shows that the model made no correct predictions for class -1 and only a few for classes 0 and 1 on the test set.")
print("This indicates that the added features and SMOTE did not significantly improve the model's ability to distinguish between the classes on unseen data.")

# Evaluate cross-validation results of the refined model
print("\nAnalysis of Cross-validation Results (Refined Model with Advanced Features and SMOTE):")
print(f"Mean F1-weighted score: {cv_scores_refined.mean():.4f}")
print(f"Standard deviation of F1-weighted scores: {cv_scores_refined.std():.4f}")
print("The mean F1-weighted score from cross-validation shows a slight improvement compared to the previous iteration, but it's still low.")
print("The standard deviation remains relatively high, suggesting that the model's performance is still inconsistent across different data subsets.")

# Analyze backtesting results of the refined model
print("\nAnalysis of Backtest Performance Summary (Refined Model with Advanced Features and SMOTE):")
print(f"Total Cumulative Strategy Return (Refined Model): {strategy_total_return_refined:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")
print("The refined strategy with advanced features and SMOTE slightly underperformed the buy-and-hold benchmark in the backtest.")
print("This indicates that, despite the new features, the trading strategy based on this model did not generate superior returns during the backtest period.")

# Determine next steps for further refinement
print("\nNext Steps for Further Refinement:")
print("Based on the analysis, the refined model's performance is still not satisfactory. We need to continue iterating on the refinement process.")
print("Given the poor performance even with advanced features and SMOTE, the next steps should focus on:")
print("1. Model Exploration and Tuning: The Gradient Boosting Classifier might not be the best fit, or requires more extensive tuning.")
print("   - Explore alternative, potentially more complex, models that can capture non-linear relationships, such as neural networks (LSTMs are good for time series).")
print("   - If sticking with tree-based models, perform a more thorough hyperparameter search using a wider range of values and potentially more sophisticated techniques like Bayesian Optimization.")
print("2. Feature Engineering: While new features were added, their predictive power is questionable based on the results.")
print("   - Re-evaluate the relevance of existing features and consider removing those with low importance (if using models that provide feature importance).")
print("   - Explore alternative ways to incorporate macroeconomic data, perhaps using different lags or transformations.")
print("   - Research and implement other widely used technical indicators or custom indicators relevant to the asset.")
print("3. Handling Class Imbalance: SMOTE with k=1 might be too aggressive or not suitable for this dataset.")
print("   - Experiment with different SMOTE parameters or other oversampling/undersampling techniques (e.g., ADASYN, NearMiss).")
print("   - Investigate using class weights in the model training process to give more importance to minority classes.")
print("4. Target Definition: The current target definition might be contributing to the poor performance.")
print("   - Test different return thresholds for defining buy/sell signals.")
print("   - Consider predicting price direction instead of discrete signals.")
print("5. Time Series Cross-Validation: Implement a robust time series cross-validation approach to ensure the model is evaluated realistically on future data.")

print("\nThe plan for the next iteration is to explore alternative models, specifically focusing on algorithms better suited for time series data, and potentially revisit feature engineering and class imbalance handling based on the chosen model's requirements.")

"""**Reasoning**:
The refined model's performance is still poor. As outlined in the previous analysis, the next step is to explore alternative models better suited for time series data and potentially revisit hyperparameter tuning, feature engineering, and class imbalance handling based on the chosen model. Given the current capabilities, exploring different machine learning models like XGBoost or LightGBM, which are variations of gradient boosting and known for strong performance, is a reasonable next step before considering more complex architectures like LSTMs which might require more data and different handling. We will also perform hyperparameter tuning on the selected model.


"""
# removed invalid line
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score
# removed invalid line
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score
# removed invalid line
# Separate features and target
X = df_features.drop(columns="Signal")
y = df_features["Signal"].copy() # Create a copy to avoid SettingWithCopyWarning

# Map target labels from -1, 0, 1 to 0, 1, 2
label_mapping = {-1: 0, 0: 1, 1: 2}
y_mapped = y.map(label_mapping)

# Split the data into training and testing sets using the mapped labels
X_train, X_test, y_train_mapped, y_test_mapped = train_test_split(X, y_mapped, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE on the mapped training data
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_mapped)

# Define the XGBoost model
# Update num_class to 3 and eval_metric to 'merror' or 'mlogloss' for multi-class classification
xgb_model = XGBClassifier(objective='multi:softmax', num_class=3, random_state=42, eval_metric='merror')

# Define a parameter grid for hyperparameter tuning (keeping it the same as before)
param_grid_xgb = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5],
    'min_child_weight': [1, 3],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Use GridSearchCV for hyperparameter tuning with mapped labels
print("Performing GridSearchCV for XGBoost with mapped labels...")
# Use make_scorer with the correct average for multi-class F1-score
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, scoring=make_scorer(f1_score, average='weighted'), cv=3, verbose=1)
grid_search_xgb.fit(X_train_resampled, y_train_resampled)

# Get the best parameters and best model
best_params_xgb = grid_search_xgb.best_params_
best_xgb_model = grid_search_xgb.best_estimator_

print("\nBest hyperparameters found by GridSearchCV:", best_params_xgb)

# Predict using the best XGBoost model on the test set (mapped labels)
y_pred_mapped = best_xgb_model.predict(X_test)

# Map predicted labels back to original labels (-1, 0, 1) for evaluation
reverse_label_mapping = {0: -1, 1: 0, 2: 1}
y_pred_original = pd.Series(y_pred_mapped).map(reverse_label_mapping)
y_test_original = y_test_mapped.map(reverse_label_mapping)


# Evaluate the best XGBoost model on the test set using original labels
print("\nBest XGBoost Model Performance Report (Mapped Labels, Evaluated with Original):")
print(classification_report(y_test_original, y_pred_original))

cm_xgb = confusion_matrix(y_test_original, y_pred_original)
print("\nBest XGBoost Model Confusion Matrix (Mapped Labels, Evaluated with Original):\n")
print(cm_xgb)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Best XGBoost Model Confusion Matrix (Mapped Labels, Evaluated with Original)")
plt.show()
plt.close('all')

# Cross-validation with the best XGBoost model using mapped labels
# Use make_scorer with the correct average for multi-class F1-score
cv_scores_xgb = cross_val_score(best_xgb_model, X, y_mapped, cv=5, scoring=make_scorer(f1_score, average='weighted'))

print("\nCross-validation F1-weighted scores (Best XGBoost Model with Mapped Labels):", cv_scores_xgb)
print("Mean F1-weighted score (Best XGBoost Model with Mapped Labels):", cv_scores_xgb.mean())
print("Standard deviation of F1-weighted scores (Best XGBoost Model with Mapped Labels):", cv_scores_xgb.std())

# Backtesting with the best XGBoost model on the original data
df_backtest_xgb = df_features.copy()
# Predict signals on the full dataset using the best model
df_backtest_xgb['Predicted Signal_Mapped'] = best_xgb_model.predict(X)
df_backtest_xgb['Predicted Signal'] = df_backtest_xgb['Predicted Signal_Mapped'].map(reverse_label_mapping)

df_backtest_xgb['Strategy Returns (XGBoost)'] = df_backtest_xgb['Return'] * df_backtest_xgb['Predicted Signal']
df_backtest_xgb['Cumulative Strategy Returns (XGBoost)'] = (1 + df_backtest_xgb['Strategy Returns (XGBoost)']).cumprod()
df_backtest_xgb['Cumulative Buy and Hold Returns'] = (1 + df_backtest_xgb['Return']).cumprod()

plt.figure(figsize=(12, 6))
plt.plot(df_backtest_xgb.index, df_backtest_xgb['Cumulative Strategy Returns (XGBoost)'], label='Strategy Returns (XGBoost)', color='blue')
plt.plot(df_backtest_xgb.index, df_backtest_xgb['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy (Best XGBoost Model) vs. Buy and Hold Benchmark')
plt.xlabel("Date")
plt.ylabel("Cumulative Returns")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all')

strategy_total_return_xgb = df_backtest_xgb['Cumulative Strategy Returns (XGBoost)'].iloc[-1] - 1
benchmark_total_return = df_backtest_xgb['Cumulative Buy and Hold Returns'].iloc[-1] - 1

print("\nðŸ“ˆ Backtest Performance Summary (Best XGBoost Model):")
print(f"Total Cumulative Strategy Return (XGBoost Model): {strategy_total_return_xgb:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""**Reasoning**:
The previous command failed because XGBoost expects class labels to be non-negative integers (0, 1, 2, ...), but the target variable `y` contains labels -1, 0, and 1. We need to map these labels to 0, 1, and 2 before training the XGBoost model.


"""
# removed invalid line
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score
# removed invalid line
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score
# removed invalid line
# Separate features and target
X = df_features.drop(columns="Signal")
y = df_features["Signal"].copy() # Create a copy to avoid SettingWithCopyWarning

# Map target labels from -1, 0, 1 to 0, 1, 2
label_mapping = {-1: 0, 0: 1, 1: 2}
y_mapped = y.map(label_mapping)

# Split the data into training and testing sets using the mapped labels
X_train, X_test, y_train_mapped, y_test_mapped = train_test_split(X, y_mapped, test_size=0.3, random_state=42)

# Address class imbalance using SMOTE on the mapped training data
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_mapped)

# Define the XGBoost model
# Update num_class to 3 and eval_metric to 'merror' or 'mlogloss' for multi-class classification
xgb_model = XGBClassifier(objective='multi:softmax', num_class=3, random_state=42, eval_metric='merror')

# Define a parameter grid for hyperparameter tuning (keeping it the same as before)
param_grid_xgb = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5],
    'min_child_weight': [1, 3],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Use GridSearchCV for hyperparameter tuning with mapped labels
print("Performing GridSearchCV for XGBoost with mapped labels...")
# Use make_scorer with the correct average for multi-class F1-score
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, scoring=make_scorer(f1_score, average='weighted'), cv=3, verbose=1)
grid_search_xgb.fit(X_train_resampled, y_train_resampled)

# Get the best parameters and best model
best_params_xgb = grid_search_xgb.best_params_
best_xgb_model = grid_search_xgb.best_estimator_

print("\nBest hyperparameters found by GridSearchCV:", best_params_xgb)

# Predict using the best XGBoost model on the test set (mapped labels)
y_pred_mapped = best_xgb_model.predict(X_test)

# Map predicted labels back to original labels (-1, 0, 1) for evaluation
reverse_label_mapping = {0: -1, 1: 0, 2: 1}
y_pred_original = pd.Series(y_pred_mapped).map(reverse_label_mapping)
y_test_original = y_test_mapped.map(reverse_label_mapping)


# Evaluate the best XGBoost model on the test set using original labels
print("\nBest XGBoost Model Performance Report (Mapped Labels, Evaluated with Original):")
print(classification_report(y_test_original, y_pred_original))

cm_xgb = confusion_matrix(y_test_original, y_pred_original)
print("\nBest XGBoost Model Confusion Matrix (Mapped Labels, Evaluated with Original):\n")
print(cm_xgb)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues", xticklabels=["-1", "0", "1"], yticklabels=["-1", "0", "1"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Best XGBoost Model Confusion Matrix (Mapped Labels, Evaluated with Original)")
plt.show()
plt.close('all')

# Cross-validation with the best XGBoost model using mapped labels
# Use make_scorer with the correct average for multi-class F1-score
cv_scores_xgb = cross_val_score(best_xgb_model, X, y_mapped, cv=5, scoring=make_scorer(f1_score, average='weighted'))

print("\nCross-validation F1-weighted scores (Best XGBoost Model with Mapped Labels):", cv_scores_xgb)
print("Mean F1-weighted score (Best XGBoost Model with Mapped Labels):", cv_scores_xgb.mean())
print("Standard deviation of F1-weighted scores (Best XGBoost Model with Mapped Labels):", cv_scores_xgb.std())

# Backtesting with the best XGBoost model on the original data
df_backtest_xgb = df_features.copy()
# Predict signals on the full dataset using the best model
df_backtest_xgb['Predicted Signal_Mapped'] = best_xgb_model.predict(X)
df_backtest_xgb['Predicted Signal'] = df_backtest_xgb['Predicted Signal_Mapped'].map(reverse_label_mapping)

df_backtest_xgb['Strategy Returns (XGBoost)'] = df_backtest_xgb['Return'] * df_backtest_xgb['Predicted Signal']
df_backtest_xgb['Cumulative Strategy Returns (XGBoost)'] = (1 + df_backtest_xgb['Strategy Returns (XGBoost)']).cumprod()
df_backtest_xgb['Cumulative Buy and Hold Returns'] = (1 + df_backtest_xgb['Return']).cumprod()

plt.figure(figsize=(12, 6))
plt.plot(df_backtest_xgb.index, df_backtest_xgb['Cumulative Strategy Returns (XGBoost)'], label='Strategy Returns (XGBoost)', color='blue')
plt.plot(df_backtest_xgb.index, df_backtest_xgb['Cumulative Buy and Hold Returns'], label='Buy and Hold Returns', color='red')
plt.title('Trading Strategy (Best XGBoost Model) vs. Buy and Hold Benchmark')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close('all')

strategy_total_return_xgb = df_backtest_xgb['Cumulative Strategy Returns (XGBoost)'].iloc[-1] - 1
benchmark_total_return = df_backtest_xgb['Cumulative Buy and Hold Returns'].iloc[-1] - 1

print("\nðŸ“ˆ Backtest Performance Summary (Best XGBoost Model):")
print(f"Total Cumulative Strategy Return (XGBoost Model): {strategy_total_return_xgb:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")

"""**Reasoning**:
Analyze the performance of the tuned XGBoost model with advanced features and SMOTE based on the classification report, confusion matrix, cross-validation, and backtesting results. Compare these results to the previous Gradient Boosting model to determine if there's an improvement and identify the next steps for refinement.


"""

# Analyze the classification report and confusion matrix of the tuned XGBoost model
print("Analysis of Classification Report (Tuned XGBoost Model):")
print("Precision, Recall, and F1-score for each class on the test set:")
print("- Class -1 (Sell): Still shows 0.00 precision, recall, and f1-score, indicating the model cannot predict sell signals on the test set.")
print("- Class 0 (Hold): Improved recall (0.75) but low precision (0.43), leading to a moderate f1-score (0.55). The model is better at identifying hold instances but still has many false positives.")
print("- Class 1 (Buy): Low precision (0.50) and recall (0.33), resulting in a low f1-score (0.40). Performance on buy signals is still weak.")

print("\nAnalysis of Confusion Matrix (Tuned XGBoost Model):")
print("The confusion matrix confirms that the model did not correctly predict any instance of class -1.")
print("For class 0, it correctly predicted 3 instances but misclassified 2 as class 1.")
print("For class 1, it correctly predicted 1 instance but misclassified 2 as class 0.")
print("Overall, the misclassification rate is still high, especially for the minority classes.")

# Evaluate cross-validation results of the tuned XGBoost model
print("\nAnalysis of Cross-validation Results (Tuned XGBoost Model):")
print(f"Mean F1-weighted score: {cv_scores_xgb.mean():.4f}")
print(f"Standard deviation of F1-weighted scores: {cv_scores_xgb.std():.4f}")
print("The mean F1-weighted score from cross-validation (0.2657) is slightly lower than the refined Gradient Boosting model (0.3027).")
print("The standard deviation (0.1423) is similar, indicating continued inconsistent performance across different data subsets.")

# Analyze backtesting results of the tuned XGBoost model
print("\nAnalysis of Backtest Performance Summary (Tuned XGBoost Model):")
print(f"Total Cumulative Strategy Return (XGBoost Model): {strategy_total_return_xgb:.4f}")
print(f"Total Cumulative Buy and Hold Return: {benchmark_total_return:.4f}")
print("The trading strategy based on the tuned XGBoost model slightly underperformed the buy-and-hold benchmark (0.0343 vs 0.0519).")
print("This suggests that the model's signals did not translate into superior returns during the backtest period.")

# Determine next steps for further refinement
print("\nNext Steps for Further Refinement:")
print("The tuned XGBoost model, despite using advanced features and SMOTE, still shows poor performance, particularly in predicting minority classes and generating superior returns in backtesting.")
print("The next steps should continue to focus on the areas identified previously:")
print("1. Revisit Class Imbalance Handling: SMOTE with k=1 might not be the optimal approach. Explore other techniques like ADASYN, NearMiss, or using class weights in the model.")
print("2. Feature Engineering: The current features might not be sufficiently informative. Explore:")
print("   - More sophisticated ways to incorporate macroeconomic data (e.g., PCA, different lags, or combining with technical indicators differently).")
print("   - Alternative technical indicators or optimizing their parameters based on the asset's characteristics.")
print("   - External data sources if available (e.g., news sentiment, fundamental data).")
print("3. Model Tuning: While GridSearchCV was used, a more extensive hyperparameter search might be needed, potentially with a wider range of values or using RandomizedSearchCV for efficiency.")
print("4. Explore Other Models: Consider other models known for handling imbalanced data or time series, such as LightGBM, or potentially more complex models if data and computational resources allow.")
print("5. Target Definition: Experiment with different thresholds for defining the buy/sell signals.")
print("6. Time Series Cross-Validation: Implement a proper time series cross-validation to get a more realistic evaluation of the model's performance on future data.")

print("\nThe plan for the next iteration is to focus on experimenting with different class imbalance handling techniques and potentially further refining the feature set, followed by re-evaluating the XGBoost model or exploring other algorithms.")

"""## Summary:

### Data Analysis Key Findings

*   The initial trading signal prediction model (RandomForestClassifier with default hyperparameters) performed poorly, particularly for the "Sell" (-1) and "Buy" (1) signals, failing to predict them at all and achieving a weighted average F1-score of only 0.45.
*   Hyperparameter tuning of the RandomForestClassifier did not significantly improve performance on the test set, with the weighted average F1-score remaining at 0.45.
*   Switching to a GradientBoostingClassifier showed a slight improvement in weighted average F1-score (0.53) and the ability to predict some instances of the minority classes, although precision and recall for these classes were still low.
*   Cross-validation with the GradientBoostingClassifier yielded a mean weighted F1-score of approximately 0.227, indicating inconsistent and generally poor performance across different data subsets.
*   Basic backtesting with the GradientBoostingClassifier showed a total cumulative strategy return of approximately 5.36%, slightly outperforming the buy-and-hold benchmark return of approximately 5.01% over the tested period.
*   Addressing class imbalance using SMOTE and incorporating advanced features (lagged macroeconomic data and interaction terms) did not lead to significant improvements in model performance on the test set, with classification reports and confusion matrices still showing poor prediction of minority classes.
*   Cross-validation with the refined model (Gradient Boosting with SMOTE and advanced features) showed a mean F1-weighted score of approximately 0.28, still indicating low and inconsistent performance.
*   Backtesting with the refined model showed a cumulative strategy return of 0.0478, slightly *underperforming* the buy-and-hold benchmark return of 0.0501.
*   Experimenting with and tuning an XGBoost model also did not result in a well-performing model, with poor prediction of minority classes and a backtest strategy that underperformed the benchmark.

### Insights or Next Steps

*   The current features and models are not sufficiently predictive of trading signals, especially for the minority classes. Further in-depth feature engineering, potentially incorporating external data or more sophisticated time series features, is crucial.
*   Experimenting with other model architectures, particularly those better suited for time series data or explicitly designed to handle class imbalance, and conducting more extensive hyperparameter tuning are necessary next steps.

"""
# removed invalid line
import matplotlib.pyplot as plt
# removed invalid line
from datetime import datetime, timedelta
# removed invalid line
# Timeframe
end_date = datetime.today()
start_date = end_date - timedelta(days=180)

# Tickers
tickers = {
    "Gold": "GC=F",
    "Oil": "CL=F",
    "S&P 500": "^GSPC",
    "USD Index": "DX-Y.NYB",
    "EUR/USD": "EURUSD=X"
}

# Download data
data = yf.download(list(tickers.values()), start=start_date, end=end_date)["Close"]
data.columns = tickers.keys()
returns = data.pct_change().dropna()

# Correlation matrix
corr_matrix = returns.corr()
print("ðŸ“ˆ Correlation Matrix:\n")
print(corr_matrix)

# Plot correlation heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Asset Return Correlation Matrix")
plt.tight_layout()
plt.show()

# Rolling correlation: Gold vs USD Index
rolling_corr_gold_usd = returns["Gold"].rolling(window=30).corr(returns["USD Index"])
plt.figure(figsize=(10, 5))
plt.plot(rolling_corr_gold_usd.index, rolling_corr_gold_usd, label="Gold vs USD Index")
plt.axhline(0, color='gray', linestyle='--')
plt.title("30-Day Rolling Correlation: Gold vs USD Index")
plt.xlabel("Date")
plt.ylabel("Correlation")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Rolling correlation: Oil vs S&P 500
rolling_corr_oil_spx = returns["Oil"].rolling(window=30).corr(returns["S&P 500"])
plt.figure(figsize=(10, 5))
plt.plot(rolling_corr_oil_spx.index, rolling_corr_oil_spx, label="Oil vs S&P 500", color='orange')
plt.axhline(0, color='gray', linestyle='--')
plt.title("30-Day Rolling Correlation: Oil vs S&P 500")
plt.xlabel("Date")
plt.ylabel("Correlation")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
# removed invalid line
from datetime import datetime, timedelta

# Step 1: Generate forward-looking forecast (simulated)
future_dates = pd.date_range(datetime.today(), periods=12, freq='W')
np.random.seed(42)

forecast_data = pd.DataFrame({
    "Gold": np.random.normal(0.002, 0.005, len(future_dates)).cumsum(),
    "Oil": np.random.normal(0.001, 0.006, len(future_dates)).cumsum(),
    "S&P 500": np.random.normal(0.003, 0.004, len(future_dates)).cumsum(),
    "USD Index": np.random.normal(-0.001, 0.003, len(future_dates)).cumsum(),
    "EUR/USD": np.random.normal(0.0005, 0.002, len(future_dates)).cumsum()
}, index=future_dates)

# Step 2: Plot forward-looking return expectations
plt.figure(figsize=(12, 6))
for asset in forecast_data.columns:
    plt.plot(forecast_data.index, forecast_data[asset], label=asset)
plt.title("Simulated Forward-Looking Asset Return Expectations")
plt.xlabel("Date")
plt.ylabel("Cumulative Expected Return")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 3: Strategic asset allocation (manually defined based on bias)
allocation = pd.Series({
    "S&P 500": 0.35,
    "Gold": 0.25,
    "Oil": 0.15,
    "EUR/USD": 0.15,
    "USD Index": 0.10
})

# Step 4: Plot asset allocation as pie chart
plt.figure(figsize=(8, 6))
allocation.plot(kind="pie", autopct="%1.1f%%", startangle=90)
plt.title("Strategic Asset Allocation Recommendation")
plt.ylabel("")
plt.tight_layout()
plt.show()

# Step 5: Directional bias summary (manually estimated)
bias = pd.DataFrame({
    "Asset": ["S&P 500", "Gold", "Oil", "USD Index", "EUR/USD"],
    "Bias": ["Bullish", "Bullish", "Neutral", "Bearish", "Neutral"]
})

print("ðŸ“Š Directional Bias Forecast")
print(bias)

import matplotlib.pyplot as plt
# removed invalid line
from datetime import datetime, timedelta
# removed invalid line
# Set time range
end_date = datetime.today()
start_date = end_date - timedelta(days=180)

# Download Volatility Index (VIX)
vix = yf.download("^VIX", start=start_date, end=end_date)["Close"]
vix.name = "VIX"

# Simulate Fear & Greed Index
np.random.seed(42)
fg_index = pd.Series(np.clip(np.random.normal(loc=50, scale=20, size=len(vix)), 0, 100),
                     index=vix.index, name="Fear & Greed Index")

# Plot VIX chart
plt.figure(figsize=(10, 5))
plt.plot(vix.index, vix, color='red', label='VIX')
plt.title("Volatility Index (VIX) - Market Risk Indicator")
plt.xlabel("Date")
plt.ylabel("VIX Level")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Fear & Greed Index
plt.figure(figsize=(10, 5))
plt.plot(fg_index.index, fg_index, color='blue', label='Fear & Greed Index')
plt.title("Simulated Fear & Greed Index - Market Sentiment")
plt.xlabel("Date")
plt.ylabel("Index Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
# removed invalid line
from IPython.display import display

# 1. Model Descriptions
models = pd.DataFrame({
    "Model/Indicator": [
        "Rolling Volatility",
        "Sharpe Ratio",
        "MACD (Moving Average Convergence Divergence)",
        "RSI (Relative Strength Index)",
        "Random Forest Classifier",
        "30-Day Rolling Correlation"
    ],
    "Description": [
        "Standard deviation of daily returns over a moving window (20 days typical). Measures risk.",
        "Average return divided by standard deviation of returns. Measures risk-adjusted return.",
        "MACD = EMA(12) - EMA(26); signals generated using crossovers with MACD signal line.",
        "Momentum oscillator that ranges from 0 to 100. RSI > 70 = overbought, < 30 = oversold.",
        "Ensemble machine learning model using multiple decision trees to predict asset signals.",
        "Measures dynamic correlation between two assets' returns using a rolling window of returns."
    ]
})

# 2. Definitions of Terms
definitions = pd.DataFrame({
    "Term": [
        "Cumulative Return",
        "Volatility",
        "Correlation",
        "Directional Bias",
        "VIX",
        "Fear & Greed Index"
    ],
    "Definition": [
        "Total return over a period including compounding of daily returns.",
        "Standard deviation of returns, used as a proxy for risk.",
        "Statistical relationship between two assets' returns (range -1 to 1).",
        "Expected short-term movement direction: Bullish / Bearish / Neutral.",
        "Volatility index showing expected 30-day S&P 500 volatility based on option prices.",
        "Sentiment gauge combining 7 indicators including momentum, volatility, and safe haven flows."
    ]
})

# 3. Disclaimer
disclaimer = """
This report is for informational and analytical purposes only and does not constitute investment advice.
Past performance is not indicative of future results. Trading financial instruments involves risk,
and you should consult your own financial advisor before making any investment decisions.
"""

# 4. References
references = [
    "Yahoo Finance API â€“ Real-time and historical price data",
    "Investopedia â€“ Financial terms and definitions",
    "sklearn â€“ Machine learning modeling library (RandomForestClassifier)",
    "matplotlib / seaborn â€“ Visualization libraries",
    "FRED (Federal Reserve Economic Data) â€“ for macroeconomic references (used in practice)"
]

# Display the three tables
print("ðŸ“Š Model Descriptions:")
display(models)
print("\nðŸ“š Glossary of Financial Terms:")
display(definitions)


# Display disclaimer and references
print("\nðŸ›‘ DISCLAIMER:")
print(disclaimer)
print("\nðŸ“š REFERENCES:")
for ref in references:
    print(f"- {ref}")

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=definitions)

# @title Model/Indicator
# removed invalid line
models.groupby('Model/Indicator').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Description
# removed invalid line
models.groupby('Description').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Model/Indicator vs Description
# removed invalid line
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['Description'].value_counts()
    for x_label, grp in models.groupby('Model/Indicator')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('Model/Indicator')
_ = plt.ylabel('Description')

from google.colab import auth
auth.authenticate_user()
print('User authenticated.')
# removed invalid line
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from statsmodels.tsa.arima.model import ARIMA
# removed invalid line
# Step 1: Download 1-year historical gold price data
end_date = datetime.today()
start_date = end_date - timedelta(days=365)
symbol = "GC=F"
gold_data = yf.download(symbol, start=start_date, end=end_date)["Close"].dropna()

# Step 2: Fit ARIMA model (p=5, d=1, q=2)
model = ARIMA(gold_data, order=(5, 1, 2))
model_fit = model.fit()

# Step 3: Forecast next 7 days
forecast = model_fit.forecast(steps=7)
forecast_dates = pd.date_range(start=gold_data.index[-1] + timedelta(days=1), periods=7)
forecast_series = pd.Series(forecast.values, index=forecast_dates)

# Step 4: Plot
plt.figure(figsize=(12, 6))
plt.plot(gold_data[-100:], label="Historical Prices")
plt.plot(forecast_series, label="7-Day Forecast", linestyle="--", marker="o", color="orange")
plt.title("Gold Price Forecast (Next 7 Days)")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 5: Print forecasted values
forecast_df = pd.DataFrame({"Forecast Price": forecast.values}, index=forecast_dates)
print(forecast_df)
# removed invalid line
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from statsmodels.tsa.api import VAR
# removed invalid line
# Step 1: Define tickers and date range
tickers = {
    "Gold": "GC=F",
    "Oil": "CL=F",
    "S&P 500": "^GSPC",
    "EUR/USD": "EURUSD=X",
    "USD Index": "DX-Y.NYB"
}

end_date = datetime.today()
start_date = end_date - timedelta(days=365)

# Step 2: Download price data
data = yf.download(list(tickers.values()), start=start_date, end=end_date)["Close"]
data.columns = tickers.keys()
data = data.dropna()

# Step 3: Compute log returns
log_returns = np.log(data / data.shift(1)).dropna()

# Step 4: Fit VAR model
model = VAR(log_returns)
results = model.fit(maxlags=15, ic='aic')

# Step 5: Forecast 7 days ahead
forecast = results.forecast(log_returns.values[-results.k_ar:], steps=7)
forecast_index = pd.date_range(start=log_returns.index[-1] + timedelta(days=1), periods=7)
forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=log_returns.columns)

# Step 6: Convert back to price estimates (from log returns)
last_prices = data.iloc[-1]
price_forecast = pd.DataFrame(index=forecast_index, columns=log_returns.columns)
for col in log_returns.columns:
    price_forecast[col] = last_prices[col] * np.exp(forecast_df[col].cumsum())

# Step 7: Plot forecasts
plt.figure(figsize=(14, 6))
for col in price_forecast.columns:
    plt.plot(price_forecast.index, price_forecast[col], label=col)
plt.title("7-Day Price Forecast (VAR Model)")
plt.xlabel("Date")
plt.ylabel("Forecasted Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 8: Display forecasted prices
print("ðŸ“Š 7-Day Price Forecasts (Final Day):")
print(price_forecast.iloc[-1].round(2))
# removed invalid line
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
# removed invalid line
# Portfolio assets and weights
tickers = {
    "Gold": "GC=F",
    "Oil": "CL=F",
    "S&P 500": "^GSPC",
    "EUR/USD": "EURUSD=X",
    "USD Index": "DX-Y.NYB"
}
weights = {
    "Gold": 0.25,
    "Oil": 0.15,
    "S&P 500": 0.35,
    "EUR/USD": 0.15,
    "USD Index": 0.10
}

# Download 3 years of data
end_date = datetime.today()
start_date = end_date - timedelta(days=3*365)
data = yf.download(list(tickers.values()), start=start_date, end=end_date)["Close"]
data.columns = tickers.keys()
data = data.dropna()

# Normalize prices to 1.0
normalized = data / data.iloc[0]

# Create portfolio value
weights_array = np.array([weights[t] for t in normalized.columns])
portfolio = (normalized * weights_array).sum(axis=1)

# Compute metrics
daily_returns = portfolio.pct_change().dropna()
cumulative_return = portfolio[-1] / portfolio[0] - 1
cagr = (portfolio[-1] / portfolio[0]) ** (1 / (len(portfolio) / 252)) - 1
volatility = daily_returns.std() * np.sqrt(252)
sharpe = (daily_returns.mean() / daily_returns.std()) * np.sqrt(252)
max_drawdown = ((portfolio / portfolio.cummax()) - 1).min()

# Plot performance
plt.figure(figsize=(12, 6))
plt.plot(portfolio, label="Portfolio")
plt.title("Backtested Portfolio Performance (3 Years)")
plt.xlabel("Date")
plt.ylabel("Normalized Value")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# Show performance metrics
metrics = pd.DataFrame({
    "Metric": ["Cumulative Return", "CAGR", "Volatility", "Sharpe Ratio", "Max Drawdown"],
    "Value": [f"{cumulative_return:.2%}", f"{cagr:.2%}", f"{volatility:.2%}", f"{sharpe:.2f}", f"{max_drawdown:.2%}"]
})
print("ðŸ“Š Portfolio Backtest Performance Metrics")
print(metrics)
# removed invalid line
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from IPython.display import display # Import display

# Step 1: Simulate key upcoming market events (replace with real calendar if needed)
event_calendar = pd.DataFrame({
    "Event Date": pd.to_datetime([
        "2025-07-10", "2025-07-12", "2025-07-15", "2025-07-18", "2025-07-20"
    ]),
    "Event": [
        "US CPI Release",
        "FOMC Rate Decision",
        "Earnings Season Kickoff",
        "Eurozone PMI",
        "China GDP Q2"
    ],
    "Expected Impact": [
        "High",
        "Very High",
        "Medium",
        "Medium",
        "High"
    ],
    "Probability of Surprise": [
        0.30, 0.40, 0.20, 0.25, 0.35
    ]
})

# Step 2: Simulated asset-class impact map
impact_map = pd.DataFrame({
    "Gold": [0.01, 0.02, 0.005, -0.004, 0.01],
    "S&P 500": [-0.01, -0.03, 0.015, 0.01, -0.02],
    "Oil": [0.00, -0.01, 0.02, 0.005, 0.01],
    "USD Index": [0.005, 0.025, -0.005, 0.002, -0.01]
}, index=event_calendar["Event"])

# Step 3: Merge both tables
event_calendar.set_index("Event", inplace=True)
calendar_impact = pd.concat([event_calendar, impact_map], axis=1)

# Step 4: Weighted forecast by probability
for asset in ["Gold", "S&P 500", "Oil", "USD Index"]:
    calendar_impact[f"{asset} Adjusted"] = calendar_impact[asset] * calendar_impact["Probability of Surprise"]

# Step 5: Plot expected impact per event
calendar_impact_plot = calendar_impact[["Gold", "S&P 500", "Oil", "USD Index"]]

calendar_impact_plot.plot(kind="bar", figsize=(12, 6), colormap="Set2")
plt.title("Expected Market Impact by Upcoming Events")
plt.ylabel("Estimated % Move")
plt.xlabel("Event")
plt.xticks(rotation=30, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 6: Show the merged impact forecast table
display(calendar_impact) # Replace ace_tools.display_dataframe_to_user with display
# removed invalid line
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from IPython.display import display # Import display

# Step 1: Simulate key upcoming market events (replace with real calendar if needed)
event_calendar = pd.DataFrame({
    "Event Date": pd.to_datetime([
        "2025-07-10", "2025-07-12", "2025-07-15", "2025-07-18", "2025-07-20"
    ]),
    "Event": [
        "US CPI Release",
        "FOMC Rate Decision",
        "Earnings Season Kickoff",
        "Eurozone PMI",
        "China GDP Q2"
    ],
    "Expected Impact": [
        "High",
        "Very High",
        "Medium",
        "Medium",
        "High"
    ],
    "Probability of Surprise": [
        0.30, 0.40, 0.20, 0.25, 0.35
    ]
})

# Step 2: Simulated asset-class impact map
impact_map = pd.DataFrame({
    "Gold": [0.01, 0.02, 0.005, -0.004, 0.01],
    "S&P 500": [-0.01, -0.03, 0.015, 0.01, -0.02],
    "Oil": [0.00, -0.01, 0.02, 0.005, 0.01],
    "USD Index": [0.005, 0.025, -0.005, 0.002, -0.01]
}, index=event_calendar["Event"])

# Step 3: Merge both tables
event_calendar.set_index("Event", inplace=True)
calendar_impact = pd.concat([event_calendar, impact_map], axis=1)

# Step 4: Weighted forecast by probability
for asset in ["Gold", "S&P 500", "Oil", "USD Index"]:
    calendar_impact[f"{asset} Adjusted"] = calendar_impact[asset] * calendar_impact["Probability of Surprise"]

# Step 5: Plot expected impact per event
calendar_impact_plot = calendar_impact[["Gold", "S&P 500", "Oil", "USD Index"]]

calendar_impact_plot.plot(kind="bar", figsize=(12, 6), colormap="Set2")
plt.title("Expected Market Impact by Upcoming Events")
plt.ylabel("Estimated % Move")
plt.xlabel("Event")
plt.xticks(rotation=30, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 6: Show the merged impact forecast table
display(calendar_impact)
# removed invalid line
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from datetime import datetime, timedelta
# removed invalid line
# Define asset and factors
asset_symbol = "^GSPC"  # Asset being analyzed
factors = {
    "Market": "^GSPC",      # Market (proxy for beta)
    "Size": "IJR",          # Small cap (Size)
    "Value": "IVE",         # Value ETF
    "Momentum": "MTUM",     # Momentum ETF
    "Quality": "QUAL"       # Quality ETF
}

# Download historical prices
end = datetime.today()
start = end - timedelta(days=365)
# Get unique tickers to download
all_tickers = list(factors.values())
unique_tickers = list(set(all_tickers)) # Use set to get unique tickers

data = yf.download(unique_tickers, start=start, end=end)["Close"].dropna()

# Rename columns to factor names
ticker_to_factor_map = {v: k for k, v in factors.items()}
data.rename(columns=ticker_to_factor_map, inplace=True)

# Identify the asset column based on the asset_symbol and its corresponding factor name
asset_column_name_in_data = ticker_to_factor_map.get(asset_symbol, asset_symbol)

# Ensure the asset column exists in the data after renaming
if asset_column_name_in_data not in data.columns:
    raise ValueError(f"Asset symbol {asset_symbol} or its factor name {asset_column_name_in_data} not found in downloaded data columns: {data.columns}")

# Rename the asset column to 'Asset' if it's not already
if asset_column_name_in_data == "Asset":
    data.rename(columns={asset_column_name_in_data: "Asset"}, inplace=True)

# Ensure 'Asset' is the first column and all factor columns are present
ordered_columns = ["Asset"] + [col for col in factors.keys() if col == "Market"] # Assuming Market is the asset itself
if "Market" in factors.keys() and asset_symbol == factors["Market"]:
     ordered_columns = ["Asset"] + [col for col in factors.keys() if col == "Market"]
else:
     ordered_columns = ["Asset"] + list(factors.keys())


# Ensure all required columns are in data before selecting
missing_columns = [col for col in ordered_columns if col not in data.columns]
if missing_columns:
    raise ValueError(f"Missing required columns in downloaded data: {missing_columns}")

data = data[ordered_columns]


# Compute log returns
returns = np.log(data / data.shift(1)).dropna()

# Prepare regression input
# X should contain only the factor columns, y should contain the asset column
# Ensure X uses the correct factor names as column names
X_columns = [col for col in ordered_columns if col == "Asset"]
X = returns[X_columns]
y = returns["Asset"]


# Fit linear regression
model = LinearRegression().fit(X, y)
coefficients = pd.Series(model.coef_, index=X.columns)
intercept = model.intercept_
r2 = model.score(X, y)

# Plot bar chart of betas
plt.figure(figsize=(8, 5))
coefficients.plot(kind='bar', color='steelblue', edgecolor='black')
plt.title(f"Factor Exposure ({asset_symbol})")
plt.ylabel("Beta Coefficient")
plt.grid(True)
plt.tight_layout()
plt.show()

# Output factor exposure table
exposure = pd.DataFrame({
    "Factor": coefficients.index,
    "Beta": coefficients.values
}).set_index("Factor")
exposure["Intercept"] = intercept
exposure["R-Squared"] = r2

print("ðŸ“Š Factor Exposure Table:")
print(exposure)
# removed invalid line
import matplotlib.pyplot as plt
# removed invalid line
from shapely.geometry import Point
# removed invalid line
# Step 1: Define global hotspots
risk_data = pd.DataFrame({
    "Region": [
        "Middle East", "Ukraine", "South China Sea", "Taiwan Strait",
        "West Africa", "US Domestic Politics"
    ],
    "Latitude": [30.0, 48.4, 12.0, 23.5, 8.5, 38.9],
    "Longitude": [45.0, 31.2, 114.0, 121.0, -11.5, -77.0],
    "Risk Type": [
        "Iran-Israel Conflict",
        "Russia-NATO Tensions",
        "China Sea Militarization",
        "Taiwan Independence Standoff",
        "Coup Risks & Militant Activity",
        "Election Volatility & Policy Shifts"
    ],
    "Impacted Markets": [
        "Oil, Gold, FX Volatility",
        "Natural Gas, European Equities",
        "Shipping, Commodities, Tech",
        "Semiconductors, Asia Equities",
        "Mining, Emerging Markets",
        "Rates, USD, Equities"
    ]
})

# Step 2: Create GeoDataFrame
gdf = gpd.GeoDataFrame(
    risk_data,
    geometry=[Point(xy) for xy in zip(risk_data["Longitude"], risk_data["Latitude"])],
    crs="EPSG:4326"
)

# Step 3: Load world map data using geopandas built-in dataset (may be deprecated but might work)
# Fallback approach if direct download fails
try:
    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
except Exception as e:
    print(f"Could not load naturalearth_lowres from built-in datasets: {e}")
    print("Attempting to download from an alternative source or you may need to manually download.")
    # If the built-in dataset still fails, you might need to provide instructions for manual download and upload.
    # For this example, we'll stop here if the built-in dataset fails.
    world = None
    # Add instructions for manual download if needed in a real scenario


if world is not None:
    # Step 4: Plot world map and hotspots
    fig, ax = plt.subplots(figsize=(14, 8))
    world.plot(ax=ax, color="lightgrey", edgecolor="white")
    gdf.plot(ax=ax, color="red", markersize=100)

    # Step 5: Add labels
    for x, y, label in zip(gdf.geometry.x, gdf.geometry.y, gdf["Region"]):
        ax.text(x + 3, y + 3, label, fontsize=9, fontweight="bold")

    plt.title("Geopolitical & Policy Risk Map â€“ Global Hotspots", fontsize=14)
    plt.axis("off")
    plt.tight_layout()
    plt.show()

    # Optional: Display table
    print("ðŸ—ºï¸ Geopolitical Risk Hotspots Table")
    print(risk_data)
else:
    print("Failed to load world map data. Cannot proceed with plotting.")
# removed invalid line
# --- Sentiment & Flow Analysis Section (Google Colab Ready) ---
# removed invalid line
import matplotlib.pyplot as plt

# Step 1: Simulate 30 days of sentiment and flow data
dates = pd.date_range(end=pd.Timestamp.today(), periods=30, freq="D")
np.random.seed(42)

data = pd.DataFrame({
    "Retail Sentiment Index": np.clip(np.random.normal(50, 10, size=len(dates)), 20, 80),
    "ETF Equity Inflows ($B)": np.random.normal(2.5, 1.0, size=len(dates)),
    "Put/Call Ratio": np.random.normal(0.9, 0.2, size=len(dates)),
    "VIX": np.random.normal(17, 3, size=len(dates))
}, index=dates)

# --- Chart 1: Retail Sentiment Index ---
plt.figure(figsize=(10, 4))
plt.plot(data.index, data["Retail Sentiment Index"], marker='o')
plt.axhline(70, color='red', linestyle='--', label="Extreme Greed")
plt.axhline(30, color='green', linestyle='--', label="Extreme Fear")
plt.title("Retail Sentiment Index (0â€“100)")
plt.ylabel("Sentiment Level")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# --- Chart 2: ETF Flows ---
plt.figure(figsize=(10, 4))
plt.bar(data.index, data["ETF Equity Inflows ($B)"], color="skyblue")
plt.axhline(0, color="black", linestyle="--")
plt.title("Daily Equity ETF Flows")
plt.ylabel("Billions USD")
plt.tight_layout()
plt.grid(True)
plt.show()

# --- Chart 3: Put/Call Ratio ---
plt.figure(figsize=(10, 4))
plt.plot(data.index, data["Put/Call Ratio"], marker="x", color="purple")
plt.axhline(1.0, linestyle='--', color='gray', label="Neutral")
plt.title("Put/Call Ratio (Sentiment Proxy)")
plt.ylabel("Ratio")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# --- Chart 4: VIX Index ---
plt.figure(figsize=(10, 4))
plt.plot(data.index, data["VIX"], color="orange", marker='.')
plt.title("VIX Index - Market Volatility Sentiment")
plt.ylabel("VIX Level")
plt.grid(True)
plt.tight_layout()
plt.show()

# --- Display Data Table ---
print("ðŸ“‹ Sentiment & Flow Data (Last 5 Days):")
print(data.tail())

# ðŸ“ Section 14: Geopolitical & Policy Risk Map
# removed invalid line
import matplotlib.pyplot as plt
# removed invalid line
# Define risk impact scores (1â€“5) for each geopolitical event on asset classes
risk_matrix = pd.DataFrame({
    "Gold": [5, 3, 2, 4, 3, 1],
    "Oil": [5, 4, 3, 2, 4, 2],
    "USD": [3, 5, 2, 3, 2, 4],
    "S&P 500": [2, 4, 3, 5, 3, 5],
    "Emerging Markets": [2, 3, 5, 3, 5, 2]
}, index=[
    "Middle East Conflict",
    "Russia-NATO Tension",
    "South China Sea",
    "Taiwan Strait",
    "West Africa Instability",
    "US Political Risk"
])

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(risk_matrix, annot=True, cmap="Reds", fmt="d", linewidths=0.5)
plt.title("Geopolitical Risk Impact Heatmap on Key Markets")
plt.ylabel("Geopolitical Risk")
plt.xlabel("Market")
plt.tight_layout()
plt.show()

# ðŸ“ Section 15: Sentiment & Flow Analysis
# removed invalid line
import matplotlib.pyplot as plt

# Simulate 30 days of sentiment data
dates = pd.date_range(end=pd.Timestamp.today(), periods=30, freq="D")
np.random.seed(42)

data = pd.DataFrame({
    "Retail Sentiment Index": np.clip(np.random.normal(50, 10, size=len(dates)), 20, 80),
    "ETF Equity Inflows ($B)": np.random.normal(2.5, 1.0, size=len(dates)),
    "Put/Call Ratio": np.random.normal(0.9, 0.2, size=len(dates)),
    "VIX": np.random.normal(17, 3, size=len(dates))
}, index=dates)

# Retail Sentiment Chart
plt.figure(figsize=(10, 4))
plt.plot(data.index, data["Retail Sentiment Index"], marker='o')
plt.axhline(70, color='red', linestyle='--', label="Extreme Greed")
plt.axhline(30, color='green', linestyle='--', label="Extreme Fear")
plt.title("Retail Sentiment Index (0â€“100)")
plt.ylabel("Sentiment Level")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# ETF Flows Chart
plt.figure(figsize=(10, 4))
plt.bar(data.index, data["ETF Equity Inflows ($B)"], color="skyblue")
plt.axhline(0, color="black", linestyle="--")
plt.title("Daily Equity ETF Flows")
plt.ylabel("Billions USD")
plt.tight_layout()
plt.grid(True)
plt.show()

# Put/Call Ratio Chart
plt.figure(figsize=(10, 4))
plt.plot(data.index, data["Put/Call Ratio"], marker="x", color="purple")
plt.axhline(1.0, linestyle='--', color='gray', label="Neutral")
plt.title("Put/Call Ratio (Sentiment Proxy)")
plt.ylabel("Ratio")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# VIX Volatility Chart
plt.figure(figsize=(10, 4))
plt.plot(data.index, data["VIX"], color="orange", marker='.')
plt.title("VIX Index - Market Volatility Sentiment")
plt.ylabel("VIX Level")
plt.grid(True)
plt.tight_layout()
plt.show()

# Show final few rows
data.tail()

# ðŸ“¦ Install required packages (only needed in Colab)
# removed invalid line

# ðŸ“ˆ Import libraries
# removed invalid line
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# ðŸŸ¡ Step 1: Download historical gold prices using GLD ETF
gold = yf.download('GLD', start='2022-01-01', end=pd.Timestamp.today().strftime('%Y-%m-%d'))['Close']

# ðŸ§  Step 2: Fit ARIMA model (order can be optimized later)
model = ARIMA(gold, order=(5, 1, 0))  # (p=5, d=1, q=0)
model_fit = model.fit()

# ðŸ”® Step 3: Forecast for the next 7 days
forecast = model_fit.forecast(steps=7)

# ðŸ“Š Step 4: Plot historical and forecasted gold prices
plt.figure(figsize=(12, 6))
plt.plot(gold[-100:], label='Historical (Last 100 Days)')
plt.plot(pd.date_range(start=gold.index[-1] + pd.Timedelta(days=1), periods=7),
         forecast, label='Forecast', marker='o', color='orange')
plt.title('Gold Price Forecast (Next 7 Days) - ARIMA Model')
plt.xlabel('Date')
plt.ylabel('Price (USD)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ðŸ§¾ Step 5: Display forecasted values
forecast

# ðŸ“¦ Install necessary packages (only in Colab)
# removed invalid line

# ðŸ“ˆ Import libraries
# removed invalid line
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# ðŸ”¢ Step 1: Define the assets in the portfolio
assets = ['GLD', 'SPY', 'TLT', 'QQQ', 'XLE']  # Gold, S&P500, Bonds, Tech, Energy
data = yf.download(assets, start="2021-01-01")['Close']
returns = data.pct_change().dropna()

# ðŸŽ¯ Step 2: Define functions for portfolio return, volatility, and Sharpe Ratio
def portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.01):
    returns = np.dot(weights, mean_returns)
    volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    sharpe = (returns - risk_free_rate) / volatility
    return returns, volatility, sharpe

def neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.01):
    return -portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate)[2]

def minimize_volatility(weights, mean_returns, cov_matrix):
    return portfolio_performance(weights, mean_returns, cov_matrix)[1]

# ðŸŽ›ï¸ Step 3: Constraints and bounds
constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
bounds = tuple((0, 1) for asset in assets)
init_guess = len(assets) * [1. / len(assets)]

mean_returns = returns.mean() * 252
cov_matrix = returns.cov() * 252

# ðŸ† Step 4: Optimization
# Max Sharpe Ratio
opt_sharpe = minimize(neg_sharpe_ratio, init_guess, args=(mean_returns, cov_matrix),
                      method='SLSQP', bounds=bounds, constraints=constraints)

# Min Variance
opt_min_var = minimize(minimize_volatility, init_guess, args=(mean_returns, cov_matrix),
                       method='SLSQP', bounds=bounds, constraints=constraints)

# ðŸ“Š Step 5: Display optimal allocations
weights_sharpe = opt_sharpe.x.round(4)
weights_min_var = opt_min_var.x.round(4)

df_weights = pd.DataFrame({
    'Asset': assets,
    'Max Sharpe Allocation': weights_sharpe,
    'Min Variance Allocation': weights_min_var
})

print(df_weights)

# ðŸŒˆ Step 6: Visualize allocations
df_weights.set_index('Asset').plot(kind='bar', figsize=(10, 5), title='Optimal Portfolio Allocations')
plt.ylabel('Weight')
plt.grid(True)
plt.tight_layout()
plt.show()
# removed invalid line
import matplotlib.pyplot as plt

risk_data = pd.DataFrame({
    'US': [0.2],
    'China': [0.6],
    'Russia': [0.8],
    'Iran': [0.7],
    'EU': [0.4],
    'Israel': [0.5],
    'Saudi Arabia': [0.3]
}).T
risk_data.columns = ['Risk Score']

plt.figure(figsize=(8, 4))
sns.heatmap(risk_data, annot=True, cmap='coolwarm', cbar=False)
plt.title("Geopolitical Risk Heatmap")
plt.tight_layout()
plt.show()
# removed invalid line
import matplotlib.pyplot as plt

data = yf.download(['^VIX', 'SPY'], start='2022-01-01')['Close']
data.dropna().plot(figsize=(10, 5), title='Market Sentiment (VIX vs SPY)')
plt.grid(True)
plt.show()

"""After running the authentication cell above and following the instructions, you should be able to run the cell with `sheets.InteractiveSheet` without the credential error."""

from statsmodels.tsa.arima.model import ARIMA
# removed invalid line
import matplotlib.pyplot as plt

gold = yf.download('GLD', start='2022-01-01')['Close']
model = ARIMA(gold, order=(5, 1, 0))
model_fit = model.fit()
forecast = model_fit.forecast(steps=7)

plt.figure(figsize=(10,5))
plt.plot(gold[-100:], label='Historical')
plt.plot(pd.date_range(start=gold.index[-1] + pd.Timedelta(days=1), periods=7), forecast, label='Forecast', color='orange')
plt.title('Gold Price Forecast (7 Days)')
plt.legend()
plt.grid(True)
plt.show()
# removed invalid line
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import yfinance as yf # Import yfinance

assets = ['GLD', 'SPY', 'TLT', 'QQQ', 'XLE']
data = yf.download(assets, start="2021-01-01")['Close']
returns = data.pct_change().dropna()

mean_returns = returns.mean() * 252
cov_matrix = returns.cov() * 252
bounds = tuple((0, 1) for _ in assets)
constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
init_guess = len(assets) * [1. / len(assets)]

def portfolio_perf(weights, mean_returns, cov_matrix, rf=0.01):
    r = np.dot(weights, mean_returns)
    vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    sharpe = (r - rf) / vol
    return r, vol, sharpe

def neg_sharpe(weights, mean_returns, cov_matrix, rf=0.01):
    return -portfolio_perf(weights, mean_returns, cov_matrix, rf)[2]

opt_sharpe = minimize(neg_sharpe, init_guess, args=(mean_returns, cov_matrix),
                      method='SLSQP', bounds=bounds, constraints=constraints)

df_alloc = pd.DataFrame({'Asset': assets, 'Allocation': opt_sharpe.x.round(4)})
df_alloc.set_index('Asset').plot(kind='bar', figsize=(10,5), title='Max Sharpe Allocation')
plt.grid(True)
plt.tight_layout()
plt.show()
df_alloc

tickers = ['GLD', 'SPY', 'TLT', 'XLE']
prices = yf.download(tickers, start='2022-01-01')['Close']
shock = {'SPY': -0.05, 'TLT': 0.01, 'GLD': 0.02, 'XLE': -0.04}
latest_prices = prices.iloc[-1]
shocked = latest_prices * (1 + pd.Series(shock))
impact = shocked - latest_prices
impact_df = pd.DataFrame({'Latest': latest_prices, 'Shocked': shocked, 'Impact ($)': impact})

print("ðŸ“‰ Simulated Shock Impact")
print(impact_df.round(2))

impact_df[['Latest', 'Shocked']].plot(kind='bar', figsize=(10,5), title='Stress Test Impact')
plt.grid(True)
plt.tight_layout()
plt.show()

# ðŸ“¦ Install Prophet (only in Google Colab)
# removed invalid line

# ðŸ“ˆ Import libraries
# removed invalid line
import matplotlib.pyplot as plt
# removed invalid line
# ðŸŸ¡ Step 1: Download Gold prices (GLD ETF as proxy)
gold = yf.download('GLD', start='2020-01-01')['Close'].reset_index()
gold = gold.rename(columns={"Date": "ds", "Close": "y"})

# ðŸ§  Step 2: Fit Prophet model
model = Prophet(daily_seasonality=False, yearly_seasonality=True, weekly_seasonality=True)
model.fit(gold)

# ðŸ”® Step 3: Forecast next 30 days
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)

# ðŸ“Š Step 4: Plot forecast
fig1 = model.plot(forecast)
plt.title("Gold Forecast (Next 30 Days) using Prophet")
plt.grid(True)
plt.show()

# ðŸ“Š Step 5: Plot forecast components
fig2 = model.plot_components(forecast)
plt.tight_layout()
plt.show()

# ðŸ” View forecasted values
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(10)